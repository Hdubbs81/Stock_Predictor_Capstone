{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a6cbcc6",
   "metadata": {},
   "source": [
    "# 4. Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd34ae1",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "# Table of Contents  \n",
    "4.1. [Introduction](#introduction)  \n",
    "4.2. [Imports](#imports)   \n",
    "4.3. [Load Pre-processed Data](#load)     \n",
    "4.4. [Build and Compile the LSTM Model](#build)       \n",
    "4.5. [Model Training](#model)       \n",
    "4.6. [Model Evaluation](#eval)      \n",
    "4.7. [Model Results](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330a1dd3",
   "metadata": {},
   "source": [
    "## 4.1 Introduction<a id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ca4de",
   "metadata": {},
   "source": [
    "The goal of this notebook is to develop a final model that effectively predicts stock market prices for the following stocks: <br>\n",
    "\t#\tStock Name/Ref<br>\n",
    "\t1)\tThe Estée Lauder Companies Inc. (EL)<br>\n",
    "\t2)\tUlta Beauty, Inc. (ULTA)<br>\n",
    "\t3)\tCOTY (COTY)<br>\n",
    "\t4)\te.l.f. Beauty, Inc. (ELF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0edf0d",
   "metadata": {},
   "source": [
    "## 4.2 Imports<a id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ba527d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 13:33:50.104992: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "#ignore warning messages to ensure clean outputs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93a18ba",
   "metadata": {},
   "source": [
    "## 4.3 Load Pre-processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46d1ecb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "X_train shape: (12544, 50, 6), dtype: object\n",
      "X_test shape: (3136, 50, 6), dtype: object\n",
      "y_train shape: (12544,), dtype: float64\n",
      "y_test shape: (3136,), dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/heatheradler/Documents/GitHub/Springboard/Springboard_Projects/Stock_Predictor_Capstone/Updated_df.csv')\n",
    "with open('/Users/heatheradler/Documents/GitHub/Springboard/Springboard_Projects/Stock_Predictor_Capstone/scalers.pkl', 'rb') as f:\n",
    "    scalers = pickle.load(f)\n",
    "\n",
    "X_train = np.load('X_train.npy', allow_pickle=True)\n",
    "X_test = np.load('X_test.npy', allow_pickle=True)\n",
    "y_train = np.load('y_train.npy', allow_pickle=True)\n",
    "y_test = np.load('y_test.npy', allow_pickle=True)\n",
    "stock_symbols_test = np.load('stock_symbols_test.npy', allow_pickle=True)\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "# Verify the shape and dtype of the loaded arrays\n",
    "print(f\"X_train shape: {X_train.shape}, dtype: {X_train.dtype}\")\n",
    "print(f\"X_test shape: {X_test.shape}, dtype: {X_test.dtype}\")\n",
    "print(f\"y_train shape: {y_train.shape}, dtype: {y_train.dtype}\")\n",
    "print(f\"y_test shape: {y_test.shape}, dtype: {y_test.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e571073f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove stock symbols from the feature arrays if present\n",
    "# Assuming stock symbols were in the last column\n",
    "if isinstance(X_train[0, 0, -1], str):\n",
    "    X_train = X_train[:, :, :-1]\n",
    "    X_test = X_test[:, :, :-1]\n",
    "\n",
    "# Convert data types to float32\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ace51eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Date = pd.to_datetime(df.Date)\n",
    "df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e11b259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 15880 entries, 1995-11-17 to 2024-03-28\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Open          15880 non-null  float64\n",
      " 1   High          15880 non-null  float64\n",
      " 2   Low           15880 non-null  float64\n",
      " 3   Close         15880 non-null  float64\n",
      " 4   Volume        15880 non-null  int64  \n",
      " 5   stock_symbol  15880 non-null  object \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 868.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f722f170",
   "metadata": {},
   "source": [
    "## 4.4 Building and Compiling the LSTM Model<a id=\"modelling\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24256e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac83cc0",
   "metadata": {},
   "source": [
    "## 4.5 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb964d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f.set_index('Date', inplace=True)\n",
    "#dates = df.index\n",
    "\n",
    "# Check the first few dates to ensure they are correct\n",
    "#print(dates[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "374fc81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cbc5689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m392/392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 0.0784 - val_loss: 0.0172\n",
      "Epoch 2/10\n",
      "\u001b[1m392/392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - loss: 0.0139 - val_loss: 0.0103\n",
      "Epoch 3/10\n",
      "\u001b[1m392/392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - loss: 0.0117 - val_loss: 0.0080\n",
      "Epoch 4/10\n",
      "\u001b[1m392/392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - loss: 0.0107 - val_loss: 0.0089\n",
      "Epoch 5/10\n",
      "\u001b[1m392/392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - loss: 0.0096 - val_loss: 0.0070\n",
      "Epoch 6/10\n",
      "\u001b[1m392/392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - loss: 0.0092 - val_loss: 0.0058\n",
      "Epoch 7/10\n",
      "\u001b[1m392/392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - loss: 0.0094 - val_loss: 0.0084\n",
      "Epoch 8/10\n",
      "\u001b[1m392/392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - loss: 0.0097 - val_loss: 0.0059\n",
      "Epoch 9/10\n",
      "\u001b[1m392/392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - loss: 0.0097 - val_loss: 0.0051\n",
      "Epoch 10/10\n",
      "\u001b[1m392/392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - loss: 0.0089 - val_loss: 0.0083\n",
      "Train Loss: 0.004591950215399265\n",
      "Test Loss: 0.00828995369374752\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Train Loss: {train_loss}\")\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e7cc7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save('/Users/heatheradler/Documents/GitHub/Springboard/Springboard_Projects/Stock_Predictor_Capstone/stock_prediction_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1f822f",
   "metadata": {},
   "source": [
    "## 4.6 Model Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25ea52c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "647ce3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbaa1cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (1295,1) doesn't match the broadcast shape (1295,5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     stock_predictions \u001b[38;5;241m=\u001b[39m predictions[stock_indices]\n\u001b[1;32m     31\u001b[0m     stock_y_test \u001b[38;5;241m=\u001b[39m y_test[stock_indices]\n\u001b[0;32m---> 33\u001b[0m     predictions_rescaled\u001b[38;5;241m.\u001b[39mextend(inverse_transform_column(stock_predictions, stock_scaler, target_col_idx))\n\u001b[1;32m     34\u001b[0m     y_test_rescaled\u001b[38;5;241m.\u001b[39mextend(inverse_transform_column(stock_y_test, stock_scaler, target_col_idx))\n\u001b[1;32m     36\u001b[0m predictions_rescaled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(predictions_rescaled)\n",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m, in \u001b[0;36minverse_transform_column\u001b[0;34m(scaled_values, scaler, column_idx)\u001b[0m\n\u001b[1;32m     10\u001b[0m placeholder[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m scaled_values[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Inverse transform the scaled values using the scaler\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m inversed \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(placeholder)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Extract the desired column after inverse transformation\u001b[39;00m\n\u001b[1;32m     16\u001b[0m inversed_column \u001b[38;5;241m=\u001b[39m inversed[:, column_idx]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:1064\u001b[0m, in \u001b[0;36mStandardScaler.inverse_transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_std:\n\u001b[0;32m-> 1064\u001b[0m         X \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n\u001b[1;32m   1066\u001b[0m         X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (1295,1) doesn't match the broadcast shape (1295,5)"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions and y_test back to original scale\n",
    "def inverse_transform_column(scaled_values, scaler, column_idx):\n",
    "    # Create a placeholder array of zeros with the same shape as scaled_values\n",
    "    placeholder = np.zeros_like(scaled_values)\n",
    "    \n",
    "    # Assign scaled_values to the correct column of the placeholder array\n",
    "    placeholder[:, 0] = scaled_values[:, 0]\n",
    "    \n",
    "    # Inverse transform the scaled values using the scaler\n",
    "    inversed = scaler.inverse_transform(placeholder)\n",
    "    \n",
    "    # Extract the desired column after inverse transformation\n",
    "    inversed_column = inversed[:, column_idx]\n",
    "    \n",
    "    return inversed_column\n",
    "\n",
    "#target_col_idx = 0  # Index of the 'Open' column in the scaled data\n",
    "\n",
    "# Inverse transform predictions and y_test for each stock\n",
    "predictions_rescaled = []\n",
    "y_test_rescaled = []\n",
    "\n",
    "for stock in np.unique(stock_symbols_test):\n",
    "    stock_scaler = scalers[stock]\n",
    "    \n",
    "    stock_indices = stock_symbols_test == stock\n",
    "    stock_predictions = predictions[stock_indices]\n",
    "    stock_y_test = y_test[stock_indices]\n",
    "    \n",
    "    predictions_rescaled.extend(inverse_transform_column(stock_predictions, stock_scaler, target_col_idx))\n",
    "    y_test_rescaled.extend(inverse_transform_column(stock_y_test, stock_scaler, target_col_idx))\n",
    "\n",
    "predictions_rescaled = np.array(predictions_rescaled)\n",
    "y_test_rescaled = np.array(y_test_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02287c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "rmse = mean_squared_error(y_test_rescaled, predictions_rescaled, squared=False)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b23d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the rescaled predictions and labels for further analysis\n",
    "np.save('predictions_rescaled.npy', predictions_rescaled)\n",
    "np.save('y_test_rescaled.npy', y_test_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3702a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions and y_test back to original scale\n",
    "predictions_rescaled = scalers.inverse_transform(predictions)\n",
    "y_test_rescaled = scalers['stock_symbol'].inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = mean_squared_error(y_test_rescaled, predictions_rescaled, squared=False)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b18d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model (optional)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = mean_squared_error(y_test_rescaled, predictions_rescaled, squared=False)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1a16c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the index\n",
    "df = df.sort_index()\n",
    "\n",
    "# Now you can access all dates from the DataFrame\n",
    "dates = df.index\n",
    "\n",
    "# Check the first few dates to ensure they are correct\n",
    "print(dates[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180de02a",
   "metadata": {},
   "source": [
    "## 4.7 Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50505d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique years from the dates\n",
    "unique_years = np.unique([date.year for date in dates])\n",
    "\n",
    "# Generate the x-axis ticks for each quarter of each year\n",
    "quarter_ticks = []\n",
    "for year in unique_years:\n",
    "    for quarter in range(1, 5):\n",
    "        quarter_ticks.append(f'{year} Q{quarter}')\n",
    "\n",
    "print(quarter_ticks)  # Check the generated quarter ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1259b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dates[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d90d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale predictions and y_test back to original scale\n",
    "predictions_rescaled = scaler.inverse_transform(np.concatenate((np.zeros((predictions.shape[0], scaled_data.shape[1] - 1)), predictions), axis=1))[:, -1]\n",
    "y_test_rescaled = scaler.inverse_transform(np.concatenate((np.zeros((y_test.shape[0], scaled_data.shape[1] - 1)), y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
    "\n",
    "# Evaluate the model\n",
    "train_rmse = mean_squared_error(y_train, model.predict(X_train), squared=False)\n",
    "test_rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dates for the test set\n",
    "dates = df.index[-len(y_test):]\n",
    "\n",
    "# Trim dates to match the length of y_test_rescaled and predictions_rescaled\n",
    "dates = dates[-len(y_test_rescaled):]\n",
    "\n",
    "# Generate the x-axis ticks for each quarter of each year\n",
    "quarter_ticks = []\n",
    "for year in unique_years:\n",
    "    for quarter in range(1, 5):\n",
    "        quarter_ticks.append(f'{year} Q{quarter}')\n",
    "\n",
    "# Plot the true and predicted 'Close' prices\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dates, y_test_rescaled, label='True Close', color='blue')\n",
    "plt.plot(dates, predictions_rescaled, label='Predicted Close', color='red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.title('True vs Predicted Close Prices')\n",
    "plt.legend()\n",
    "plt.xticks(ticks=dates[::len(dates)//len(quarter_ticks)], labels=quarter_ticks, rotation=45)  # Set custom ticks and labels\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da72c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
