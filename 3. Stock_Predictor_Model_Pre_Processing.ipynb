{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1980f83d",
   "metadata": {},
   "source": [
    "# 3. Data Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809181b",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "# Table of Contents  \n",
    "3.1. [Introduction](#introduction)  \n",
    "3.2. [Imports](#imports)   \n",
    "3.3. [Data Processing](#process)  \n",
    "3.4. [Scale the Data](#data)\n",
    "3.5. [Create LSTM Sequences](#create)\n",
    "3.6. [Data Splitting](#split)\n",
    "3.7. [Save Updated Dataframe](#save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb69c9",
   "metadata": {},
   "source": [
    "## 3.1 Introduction<a id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ce9f3",
   "metadata": {},
   "source": [
    "The goal of this notebook is to create a cleaned development dataset to be used to complete the modeling step of my project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b908f48",
   "metadata": {},
   "source": [
    "## 3.2 Imports<a id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c7849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import os\n",
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4139df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/heatheradler/Documents/GitHub/Springboard/Springboard_Projects/Stock_Predictor_Capstone/Concated_Dataframe.csv')\n",
    "df = df[df['stock_symbol'].isin(['EL','ULTA','COTY','ELF'])]\n",
    "formulas_to_keep = ['stock_symbol','Date', 'Open', 'High', 'Low', 'Close','Volume', 'Dividends', 'Stock Splits', 'EMA_10', 'PSARl_0.02_0.2', 'PSARs_0.02_0.2', 'BBL_5_2.0', 'BBM_5_2.0', 'BBU_5_2.0', 'ISA_9', 'ISB_26', 'ITS_9', 'IKS_26', 'ICS_26']\n",
    "df = df[formulas_to_keep]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb086e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Date = pd.to_datetime(df.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8947b52",
   "metadata": {},
   "source": [
    "## 3.3 Data Pre-processing<a id=\"process\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7bd95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess data\n",
    "def preprocess_data(df):\n",
    "    df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'stock_symbol']].copy()\n",
    "    \n",
    "    # Set Date as index and sort\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocess the data\n",
    "df_1 = preprocess_data(df)\n",
    "print(df_1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5584ff3",
   "metadata": {},
   "source": [
    "## 3.4 Scale the Data<a id=\"data\"></a>\n",
    "\n",
    "Standardization (Z-score normalization) is used to transform the data to have a mean of 0 and a standard deviation of 1. This is to ensure optimal performance and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59578ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(df):\n",
    "    scalers = {}\n",
    "    scaled_data = pd.DataFrame()\n",
    "    \n",
    "    for stock in df['stock_symbol'].unique():\n",
    "        stock_data = df[df['stock_symbol'] == stock]\n",
    "        scaler = StandardScaler()\n",
    "        scaled_values = scaler.fit_transform(stock_data.drop(columns='stock_symbol'))\n",
    "        scaled_df = pd.DataFrame(scaled_values, columns=stock_data.columns[:-1], index=stock_data.index)\n",
    "        scaled_df['stock_symbol'] = stock\n",
    "        scalers[stock] = scaler\n",
    "        scaled_data = pd.concat([scaled_data, scaled_df])\n",
    "        \n",
    "    return scaled_data, scalers\n",
    "\n",
    "scaled_df, scalers = scale_data(df_1)\n",
    "print(scaled_df.head())\n",
    "\n",
    "# Save the scalers\n",
    "with open('/Users/heatheradler/Documents/GitHub/Springboard/Springboard_Projects/Stock_Predictor_Capstone/scalers.pkl', 'wb') as f:\n",
    "    pickle.dump(scalers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb209d4",
   "metadata": {},
   "source": [
    "## 3.5 Create LSTM Sequences<a id=\"create\"></a>\n",
    "\n",
    "Long Short-Term Memory (LSTM) models are designed to work with sequential data, therefore sequences must be created for training. Creating LSTM sequences is a crucial step in preparing data for stock market prediction because it helps the model understand the patterns and trends over time. Specifically, creating LSTM sequences are important for: \n",
    "\n",
    "1) Understanding Trends: Stock prices are influenced by their historical values. By creating sequences, we allow the model to look at a series of past prices and learn how these prices evolve over time.\n",
    "\n",
    "2) Capturing Patterns: Financial data often shows specific patterns, like trends or cycles. Sequences help the model recognize these patterns by providing context from previous days or weeks.\n",
    "\n",
    "3) Improving Predictions: Just looking at a single day's data isn't enough to make accurate predictions. Sequences give the model a broader view, allowing it to make better-informed predictions about future prices.\n",
    "\n",
    "4) Handling Time Dependency: Stock prices are inherently time-dependent; today's price is related to yesterday's price. Sequences ensure that this time dependency is captured in the model, which is crucial for making accurate forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2cbb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i+seq_length]\n",
    "        label = data[i+seq_length][3]  # The 'Close' price is the 4th column (0-based index)\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "        \n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# Define the sequence length\n",
    "SEQ_LENGTH = 50\n",
    "\n",
    "# Filter by stock symbol and create sequences\n",
    "all_sequences = []\n",
    "all_labels = []\n",
    "\n",
    "for stock in scaled_df['stock_symbol'].unique():\n",
    "    stock_data = scaled_df[scaled_df['stock_symbol'] == stock].drop(columns='stock_symbol').values\n",
    "    sequences, labels = create_sequences(stock_data, SEQ_LENGTH)\n",
    "    all_sequences.extend(sequences)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "all_sequences = np.array(all_sequences)\n",
    "all_labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46b6b2d",
   "metadata": {},
   "source": [
    "## 3.6 Data Splitting<a id=\"split\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_sequences, all_labels, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf14496",
   "metadata": {},
   "source": [
    "## 3.7 Save Updated Dataframe<a id=\"save\"></a>\n",
    "\n",
    "The training and test sets must be saved as numpy arrays to be imported into the Modeling notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736eaedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save updated dataframe\n",
    "df.to_csv('/Users/heatheradler/Documents/GitHub/Springboard/Springboard_Projects/Stock_Predictor_Capstone/Updated_df.csv')\n",
    "\n",
    "print(\"Data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a86bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to .npy files\n",
    "np.save('X_train.npy', X_train)\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('y_train.npy', y_train)\n",
    "np.save('y_test.npy', y_test)\n",
    "\n",
    "print(\"Data saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
