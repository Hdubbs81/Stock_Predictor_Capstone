{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1980f83d",
   "metadata": {},
   "source": [
    "# 3. Data Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809181b",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "# Table of Contents  \n",
    "3.1. [Introduction](#introduction) <br>\n",
    "3.2. [Imports](#imports)  <br>\n",
    "3.3. [Data Processing](#process)<br>\n",
    "3.4. [Scale the Data](#data)<br>\n",
    "3.5. [Create LSTM Sequences](#create)<br>\n",
    "3.6. [Data Splitting](#split)<br>\n",
    "3.7. [Save Updated Dataframe](#save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb69c9",
   "metadata": {},
   "source": [
    "## 3.1 Introduction<a id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ce9f3",
   "metadata": {},
   "source": [
    "The goal of this notebook is to create a cleaned development dataset to be used to complete the modeling step of my project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b908f48",
   "metadata": {},
   "source": [
    "## 3.2 Imports<a id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b32c7849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import os\n",
    "import csv\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4139df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/heatheradler/Documents/GitHub/Springboard/Springboard_Projects/Stock_Predictor_Capstone/Concated_Dataframe.csv')\n",
    "df = df[df['stock_symbol'].isin(['EL','ULTA','COTY','ELF'])]\n",
    "formulas_to_keep = ['stock_symbol','Date', 'Open', 'High', 'Low', 'Volume', 'Dividends', 'Stock Splits', 'EMA_10', 'PSARl_0.02_0.2', 'PSARs_0.02_0.2', 'BBL_5_2.0', 'BBM_5_2.0', 'BBU_5_2.0', 'ISA_9', 'ISB_26', 'ITS_9', 'IKS_26', 'ICS_26']\n",
    "df = df[formulas_to_keep]\n",
    "df.head()\n",
    "scalers = pickle.load(open('scalers.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb086e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Date = pd.to_datetime(df.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e662fa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15880 entries, 0 to 15879\n",
      "Data columns (total 19 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   stock_symbol    15880 non-null  object        \n",
      " 1   Date            15880 non-null  datetime64[ns]\n",
      " 2   Open            15880 non-null  float64       \n",
      " 3   High            15880 non-null  float64       \n",
      " 4   Low             15880 non-null  float64       \n",
      " 5   Volume          15880 non-null  int64         \n",
      " 6   Dividends       15880 non-null  float64       \n",
      " 7   Stock Splits    15880 non-null  float64       \n",
      " 8   EMA_10          15880 non-null  float64       \n",
      " 9   PSARl_0.02_0.2  15880 non-null  float64       \n",
      " 10  PSARs_0.02_0.2  15880 non-null  float64       \n",
      " 11  BBL_5_2.0       15880 non-null  float64       \n",
      " 12  BBM_5_2.0       15880 non-null  float64       \n",
      " 13  BBU_5_2.0       15880 non-null  float64       \n",
      " 14  ISA_9           15880 non-null  float64       \n",
      " 15  ISB_26          15880 non-null  float64       \n",
      " 16  ITS_9           15880 non-null  float64       \n",
      " 17  IKS_26          15880 non-null  float64       \n",
      " 18  ICS_26          15880 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(16), int64(1), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8947b52",
   "metadata": {},
   "source": [
    "## 3.3 Data Pre-processing<a id=\"process\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf7bd95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Open      High       Low    Volume stock_symbol\n",
      "Date                                                           \n",
      "1995-11-17  6.086093  6.530846  6.062685  35659200           EL\n",
      "1995-11-20  6.437214  6.601070  6.132909   8434000           EL\n",
      "1995-11-21  6.179726  6.203134  5.945645   6440000           EL\n",
      "1995-11-22  6.086094  6.390399  6.086094   3480800           EL\n",
      "1995-11-24  6.343582  6.530847  6.320174   1279200           EL\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess data\n",
    "def preprocess_data(df):\n",
    "    df = df[['Date', 'Open', 'High', 'Low', 'Volume', 'stock_symbol']].copy()\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df = df.sort_index()\n",
    "    return df\n",
    "\n",
    "# Preprocess the data\n",
    "df_1 = preprocess_data(df)\n",
    "print(df_1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5584ff3",
   "metadata": {},
   "source": [
    "## 3.4 Scale the Data<a id=\"data\"></a>\n",
    "\n",
    "Standardization (Z-score normalization) is used to transform the data to have a mean of 0 and a standard deviation of 1. This is to ensure optimal performance and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b59578ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Open      High       Low     Volume stock_symbol\n",
      "Date                                                            \n",
      "1995-11-17 -0.785894 -0.780930 -0.785739  18.654287           EL\n",
      "1995-11-20 -0.781538 -0.780069 -0.784858   3.551529           EL\n",
      "1995-11-21 -0.784733 -0.784949 -0.787208   2.445389           EL\n",
      "1995-11-22 -0.785894 -0.782653 -0.785445   0.803819           EL\n",
      "1995-11-24 -0.782700 -0.780930 -0.782508  -0.417485           EL\n"
     ]
    }
   ],
   "source": [
    "def scale_data(df_1):\n",
    "    scalers = {}\n",
    "    scaled_data = pd.DataFrame()\n",
    "    \n",
    "    for stock in df_1['stock_symbol'].unique():\n",
    "        stock_data = df_1[df_1['stock_symbol'] == stock]\n",
    "        scaler = StandardScaler()\n",
    "        scaled_values = scaler.fit_transform(stock_data.drop(columns='stock_symbol'))\n",
    "        scaled_df = pd.DataFrame(scaled_values, columns=stock_data.columns[:-1], index=stock_data.index)\n",
    "        scaled_df['stock_symbol'] = stock\n",
    "        scalers[stock] = scaler\n",
    "        scaled_data = pd.concat([scaled_data, scaled_df])\n",
    "        \n",
    "    return scaled_data, scalers\n",
    "\n",
    "scaled_df, scalers = scale_data(df_1)\n",
    "print(scaled_df.head())\n",
    "\n",
    "# Save the scalers\n",
    "with open('/Users/heatheradler/Documents/GitHub/Springboard/Springboard_Projects/Stock_Predictor_Capstone/scalers.pkl', 'wb') as f:\n",
    "    pickle.dump(scalers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb209d4",
   "metadata": {},
   "source": [
    "## 3.5 Create LSTM Sequences<a id=\"create\"></a>\n",
    "\n",
    "Long Short-Term Memory (LSTM) models are designed to work with sequential data, therefore sequences must be created for training. Creating LSTM sequences is a crucial step in preparing data for stock market prediction because it helps the model understand the patterns and trends over time. Specifically, creating LSTM sequences are important for: \n",
    "\n",
    "1) Understanding Trends: Stock prices are influenced by their historical values. By creating sequences, we allow the model to look at a series of past prices and learn how these prices evolve over time.\n",
    "\n",
    "2) Capturing Patterns: Financial data often shows specific patterns, like trends or cycles. Sequences help the model recognize these patterns by providing context from previous days or weeks.\n",
    "\n",
    "3) Improving Predictions: Just looking at a single day's data isn't enough to make accurate predictions. Sequences give the model a broader view, allowing it to make better-informed predictions about future prices.\n",
    "\n",
    "4) Handling Time Dependency: Stock prices are inherently time-dependent; today's price is related to yesterday's price. Sequences ensure that this time dependency is captured in the model, which is crucial for making accurate forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e2cbb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, seq_length, stock_symbol):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    stock_symbols = []\n",
    "    \n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i + seq_length, :-1]\n",
    "        label = data[i + seq_length][3]  # Use 'Open' as target variable\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "        stock_symbols.append(stock_symbol)\n",
    "        #stock_symbols.append(data[i + seq_length][-1])  # Store the stock symbol\n",
    "        \n",
    "    return np.array(sequences), np.array(labels), np.array(stock_symbols)\n",
    "\n",
    "# Define the sequence length\n",
    "SEQ_LENGTH = 50\n",
    "\n",
    "# Filter by stock symbol and create sequences\n",
    "all_sequences = []\n",
    "all_labels = []\n",
    "all_stock_symbols = []\n",
    "\n",
    "for stock in scaled_df['stock_symbol'].unique():\n",
    "    stock_data = scaled_df[scaled_df['stock_symbol'] == stock].drop(columns='stock_symbol').values\n",
    "    sequences, labels, stock_symbols = create_sequences(stock_data, SEQ_LENGTH, stock)\n",
    "    all_sequences.extend(sequences)\n",
    "    all_labels.extend(labels)\n",
    "    all_stock_symbols.extend(stock_symbols)\n",
    "\n",
    "all_sequences = np.array(all_sequences)\n",
    "all_labels = np.array(all_labels)\n",
    "all_stock_symbols = np.array(all_stock_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46b6b2d",
   "metadata": {},
   "source": [
    "## 3.6 Data Splitting<a id=\"split\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d04caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test, stock_symbols_train, stock_symbols_test = train_test_split(all_sequences, all_labels, all_stock_symbols, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Save the symbols for test set\n",
    "#np.save('symbols_test.npy', symbols_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf14496",
   "metadata": {},
   "source": [
    "## 3.7 Save Updated Dataframe<a id=\"save\"></a>\n",
    "\n",
    "The training and test sets must be saved as numpy arrays to be imported into the Modeling notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ae2baad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# save updated dataframe\n",
    "df_1.to_csv('/Users/heatheradler/Documents/GitHub/Springboard/Springboard_Projects/Stock_Predictor_Capstone/Updated_df.csv')\n",
    "\n",
    "print(\"Data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77a86bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the data to .npy files\n",
    "np.save('X_train.npy', X_train, allow_pickle=True)\n",
    "np.save('X_test.npy', X_test, allow_pickle=True)\n",
    "np.save('y_train.npy', y_train, allow_pickle=True)\n",
    "np.save('y_test.npy', y_test, allow_pickle=True)\n",
    "np.save('stock_symbols_train.npy', stock_symbols_train, allow_pickle=True)\n",
    "np.save('stock_symbols_test.npy', stock_symbols_test, allow_pickle=True)\n",
    "\n",
    "print(\"Data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1acecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
